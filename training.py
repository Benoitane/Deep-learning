# -*- coding: utf-8 -*-
"""training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SCKFuAQXaKp-uFAVD56vi6yIyjPPdOJ2
"""

import numpy as np
import pandas as pd
from sklearn.metrics import classification_report
import keras
from tensorflow.python.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Dropout, BatchNormalization
from tensorflow.python.keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Flatten, MaxPool1D
from tensorflow.keras.models import Sequential
from sklearn.feature_extraction.text import CountVectorizer
from tensorflow.python.keras import optimizers
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from keras.utils.np_utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn import preprocessing
from tensorflow.python.keras.optimizers import Adam
from keras import regularizers
from keras.utils.np_utils import to_categorical
import warnings
from sklearn.metrics import classification_report
from keras import backend as K
import matplotlib.pyplot as plt

warnings.filterwarnings('ignore')

def train_deep_modele(model, xtrain, ytrain,epochs, callbacks , batch_size ,val_data):
  #Définition du modèle
  hist = model.fit(xtrain, ytrain,epochs = epochs, callbacks = callbacks, batch_size = batch_size ,validation_data = val_data)
  
  #Plot
  fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14,5))
  ax1.plot(hist.history['loss'],label='train')
  ax1.plot(hist.history['val_loss'],label='validation')
  ax1.set_title('Evolution of training loss')
  ax1.legend()

  ax2.plot(hist.history['accuracy'],label='train')
  ax2.plot(hist.history['val_accuracy'],label='validation')
  ax2.set_title('Evolution of training accuracy')
  ax2.legend()

  plt.tight_layout()
  plt.show()





